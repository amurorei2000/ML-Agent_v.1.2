Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.0380986,23.95641155600608,-1.5913898,-9.157710312814668,-9.157710312814668,20.264057,0.022833537,0.00028462787,0.19487597,0.00474431,1.0
100000,0.749597,87.31608133086876,-2.8030114,-7.2542346710301455,-7.2542346710301455,5.162924,0.023625843,0.00025694355,0.18564786,0.0042838273,1.0
150000,0.49902374,200.51004016064257,-2.5380445,-0.4484415584539314,-0.4484415584539314,2.6804903,0.022262394,0.00022615686,0.17538561,0.0037717414,1.0
200000,0.3480012,260.9319371727749,-1.7073041,3.359432175755501,3.359432175755501,2.1122613,0.023281014,0.0001953117,0.16510388,0.0032586835,1.0
250000,0.25281945,292.8362573099415,-0.89888823,5.346360723964524,5.346360723964524,1.7053545,0.024671119,0.00016446618,0.15482204,0.0027456195,1.0
300000,0.20520331,300.832298136646,-0.13730448,8.818074817474214,8.818074817474214,1.9246609,0.024012722,0.00013354735,0.14451577,0.0022313362,1.0
350000,0.16161913,310.75,0.52189314,8.85353172708701,8.85353172708701,1.7641987,0.023869097,0.00010274108,0.13424699,0.0017189251,1.0
400000,0.12805037,324.66883116883116,1.1072878,9.587645477008435,9.587645477008435,1.3017809,0.022656891,7.497953e-05,0.12499315,0.0012571581,1.0
450000,0.10172191,318.8980891719745,1.5672247,9.765220045924757,9.765220045924757,1.2173214,0.02250554,4.7233407e-05,0.11574445,0.0007956475,1.0
500000,0.088362485,317.57516339869284,1.8598065,9.557827082310432,9.557827082310432,1.3630898,0.02436405,1.6399175e-05,0.105466366,0.00028277133,1.0
